---
title: DevOps 之路
date: 2018-06-18 10:35:52
tags: 
- DevOps
---

## Devops 实施的前提条件：

软件架构解耦， 可独立部署，独立升级。

## 流水线

* DevOps之流水线

 流水线是指软件从版本控制仓库到用户手中可使用这一过程的自动化表现形式。对软件的每次变更都会经历一系列的流程才能发布，这一流程包括单元测试、构建、打包、部署，集成测试， 性能测试，安全扫描，发布等。流水线是对这一流程的建模。

* 流水线的核心理念

 基于价值的交付。基于数据的准确度量，帮助开发或者管理者评估产品价值和优化团队效率。

## 研发模式的选择

* 根据开发团队规模：小规模团队采用分支研发，共建型大团队采用 gitflow，更加庞大的团队采用主干开发模式。因为微服务设计的流行，现在应用的团队规模越来越小，所以在阿里分支研发模式会更受欢迎。
* 当某个分支功能不想发布了，分支模式可以直接退出集成，需要发布的分支重新合并 release 即可。而主干开发往往采用特性开关方式 off 掉相应功能，因为代码剥离比较困难。

> **特性开关的理解**
* 特性开关和特性分支都是解决代码并行开发的方法，帮助团队进行独立的变更发布。特性分支很容易入手，但是会引入痛苦的合并冲突。这种风险往往不利于对代码进行渐进式的优化，导致技术债务的累积并走向不归路。
* 特性开关允许团队实施真正的持续集成，并将代码更改与功能发布解耦，代价是增加了代码中开关的复杂性。特性开关不是万能的，也并非总是最佳选择，好在尝试使用这种方法对于开发团队来说成本很低，只需硬编码 if/else 语句即可，接下来的事情，就是检验是否可行。

## 环境：

### 线下环境

* 开发测试环境：这个环境主要是开发使用，能够快速验证自己开发完的代码。不需要和线上环境相同，如果有依赖，可以直接使用集成测试环境的服务。
* 集成测试环境： 这个环境的应用的各类基础服务和线上环境保持一致，只是比线上环境规模小很多。因为开发和测试都在使用，同时不能影响测试的效率，所以这个环境要求稳定。如果变更是有损的，需要严格的审核，不能随意发布新的变更。如果变更是无损的，变更审核会简单点，可以较频繁的进行变更。所以无损变更可以更加充分的利用该环境，提高环境的使用效率。

### 线上环境

* 预发环境 ：和正式生产环境的状态基础服务共用，如 DB，KV，文件存储以及搜索类的数据服务。网络上独立网段划分，不承担线上真是流量。
  >预发环境正式使用后的另一用途：就是生产环境出现问题，但是线下环境复现不了时，就可以在预发环境上复现。
* Beta环境：也叫灰度环境，金丝雀发布就是基于该环境发布。简单理解就是从生产环境的集群中，再建立一个独立集群，就是再建立一个分组，独立出一个集群，大概只有1——2台服务器数量，主要针对小规模真是业务流量。如何保证流量小规模，就需要负载均衡策略上做工作, 主要两种方式：1.调用RPC，在服务化框架的复杂均衡策略中，将其权重或者流量配比降低。2.调用HTTP，在四层VIP或者七层权重上，将其权重降低。
  > 除了承担流量不同，其他和生产环境的应用没有差别，主要针对核心应用。
* 生产环境

## 持续交付可能面临的挑战

* 缺少测试覆盖的持续交付会成为负担。当我们单元测试、API测试做的不好的时候，通过流程强加的持续交付基本上是自欺欺人，要么就是不稳定，要么就是没效果。
* 测试团队转型，开发全栈导致的质量下降。有这么多需求，没时间写测试，或者保姆式服务享受惯了，开发人员自己没这个意识。

## 压力测试系统

### 压测粒度

* 单机单应用压力测试
* 单链路压力测试
* 多链路/全链路压力测试
 > 多链路本质上就是多个单链路的组合，全链路就是多链路的组合。

### 压测接口及流量构造方式

1. 接口一般分为HTTP接口和RPC接口
2. 流量构造方式：
* 线上流量回放：
> 直接利用了线上流量模型，比较接近真实业务场景，常见的技术手段如 TCPCopy，或者 Tcpdump 抓包保存线上请求流量。但是这种方式也存在一些代价，比如需要镜像请求流量，当线上流量非常大的时候就很难全部镜像下来，而且还需要大量额外的机器来保存流量镜像。到了回放阶段，还需要一些自动化的工具来支持，还要解决各种 session 问题，真正实施的时候，还是会有不少的工作量。
* 线上流量引流：
> 既然线上回放比较麻烦，那为什么不直接使用线上流量进行压测呢？这个思路确实是可行的，我们前面讲过，压测的主要是 HTTP 和 RPC 两种类型的接口，为了保证单个应用的流量压力足够大，这里可以采取两种模式。
  > 一个是将应用集群中的流量逐步引流到一台主机上，直到达到其容量阈值；另一个方案是，可以通过修改负载均衡中某台主机的权重，将更多的流量直接打到某台主机上，直到达到其容量阈值。这个过程中，我们可以设定单台主机的 CPU、Load 或者 QPS、RT 等阈值指标，当指标超出正常阈值后就自动终止压测，这样就可以获取到初步的容量值。
  > 这种方式的好处是，不需要额外的流量模拟，直接使用最真实的线上流量，操作方便，且更加真实。
* 流量模拟
> 上述两种流量模拟方式，更适合日常单机单应用的容量压测和规划，但是对于大促这种极端业务场景，真实流量就很难模拟了，因为这种场景只有特定时刻才会有，我们在日常是无法通过线上流量构造出来的。
> 所以这里就需要利用数据工厂，最终通过流量平台来形成压测流量。这里的工具用到了 Gatling，是一款开源的压测工具，用 Scala 开发的，可以针对自己的需求，比如自动生成压测脚本等，做了一些二次开发。

### 施压方式

接下来要做的就是对真实的线上系统施加压力流量了。很自然的，这里就需要有施加压力的机器。关于施压机器的分布，大部分仍然是跟线上系统在同机房内，少量会在公有云节点上。但是对于阿里，因为其自身的 CDN 节点遍布全球，所以他就可以将全球（主要是国内）的 CDN 节点作为施压机，更加真实地模拟真实用户从全球节点进入的真实访问流量。这种方式对于中小型公司就显得成本过高，技术条件和细节也还达不到这个程度。不过当前阿里已经将这种压测能力输出到了阿里云之上。

### 数据读写

压测过程中，对于读的流量更好构造，因为读请求本身不会对线上数据造成任何变更，但是对于写流量就完全不一样了，如果处理不好，会对线上数据造成污染，对商家和用户造成资损。

所以，对于写流量就要特殊处理，这块也有比较通用的解决方案，就是对压测的写请求做专门的标记。当请求要写数据库时，由分布式数据库的中间件框架中的逻辑来判断这个请求是否是压测请求，如果是压测写请求则路由到对应的影子库中，而不是直接写到线上正式的库中。

在这之前，要提前创建好对应的影子库。假设建立影子库的原则是原 schema + mirror，如果正式库是 order，则影子库为 order_mirror，这时两个库中的数据量必须是一致的。对于非敏感信息，数据内容也可以保持一致，这样可以在最大程度上保证数据模型一致。

---
这里要重点强调一下**基础服务标准化**工作，如果这个工作在开始做得扎实，它的优势在这里就体现出来了。我们刚刚提到的影子库的路由策略是基于中间件框架来实现的，如果使用的框架不一样，不是标准的，这个功能可能就很难应用起来。